####  损失函数

+ 损失函数是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数，通常使用L(Y，f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示称如下式子：

  ![](https://i.loli.net/2019/04/08/5caabc3266e89.png)

+ 损失函数可分为以下几种

  + log对数损失函数(逻辑回归)

  + 平方损失函数(最小二乘法)

  + 指数损失函数(Adaboost)

  + Hinge损失函数(SVM)

  + 0-1损失

  + 绝对值损失

    

  + **log对数损失函数(逻辑回归)**

    Logistic回归的损失函数就是对数损失函数，在Logistic回归的推导中，它假设样本服从伯努利分布(0-1分布)，然后求得满足该分布的似然函数，接着用对数求极值。Logistic回归并没有求对数极大似然函数的最大值，而是把它当做一个思想，进而推导出它的风险函数为最小化的负的似然函数。从损失函数的角度上，它就成为了log损失函数，其标准形式：![](https://i.loli.net/2019/04/08/5caaf520e278a.png)

    Logistic回归目标式子如下：![](https://i.loli.net/2019/04/08/5caaf5a5d3d01.png)

    如果是二分类的话，则m值等于2，如果是多分类，m就是相应的类别总个数。

  + **平方损失函数(最小二乘法，Ordinary Least Squares)**

    最小二乘法是线性回归的一种方法，它讲回归的问题转化为了凸优化的问题。最小二乘法的基本原则是：最优拟合曲线应该使得所有点到回归直线的距离和最小。通常用欧式距离进行距离的度量。当样本个数为n时，此时的损失函数变为：

    ![](https://i.loli.net/2019/04/08/5caaf6e3171e4.png)

    Y-f(x)表示的是残差，整个式子表示的是残差的平方和，而我们的目标就是最小化这个目标函数值(注：该子式未加入正则项)，也就是最小化残差的平方和。

    规则基本上可以找到足够的正样本，但是对于一些极端情况，例如所有的Anchor对应的anchor box与groud truth的IoU不大于0.7,可以采用第一种规则生成。而在实际应用中，通常会使用均方差(MSE)作为一项衡量指标，公式如下：![](https://i.loli.net/2019/04/08/5caaf80e4cc61.jpg)

  + **指数损失函数(Adaboost)**

    指数损失函数(exp-loss)的标准形式如下：![](https://i.loli.net/2019/04/08/5caaff7855c76.png)

    可以看出，Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：![](https://i.loli.net/2019/04/08/5caaffe2d5f97.png)

    关于Adaboost的推导，可参考其他资料。

  + **Hinge损失函数(SVM)**

    Hinge loss用于最大间隔(maximum-margin)分类，其中最有代表性的就是支持向量机SVM。Hinge函数的标准形式：![](https://i.loli.net/2019/04/08/5cab00c5dac7f.png)

    其中，t为目标值(-1或+1)，y是分类器输出的预测值，并不能直接是类标签。其含义为，当t和y的符号相同时(表示y预测正确)并且|y|≥1时，hinge loss为0；当t和y的符号相反时，hinge loss随着y的增大线性增大。

  + **0-1损失函数**

    在分类问题中，可以使用函数的正负号来进行模式判断，函数本身的大小并不是很重要，0-1损失函数比较的是预测值与真实值的符号是否相同，0-1损失的具体形式如下：![](https://i.loli.net/2019/04/08/5cab0310cd6fd.png)

  + **绝对值损失函数**

    绝对值损失函数的意义和平方损失函数差不多，只不过是取了绝对值而不是求绝对值，差距不会被平方放大，其形式为：![](https://i.loli.net/2019/04/08/5cab03bd094a4.png)

  对于具体的某个样本点，有了衡量其预测值与真实值的差异度的方法就是选取上面任意一个损失函数即可。

+ 期望风险、经验风险与结构风险之间的关系

  首先，损失函数是期望风险、经验风险和结构风险的基础。通过损失函数我们只能知道模型决策函数f(x)对于单个样本点的预测能力（借用损失函数L(Y，f(x))，损失函数越小，说明模型对于该样本预测越准确），那么如果想知道模型f(x)对训练样本中所有的样本的预测能力应该怎么办呢？显然只需所有的样本点都求一次损失函数然后进行累加就好了。如下式：![](https://i.loli.net/2019/04/08/5caabb19ca728.png)

  这就是经验风险，所谓的经验风险最小化便是让这个式子最小化，注意这个式子中累加和的上标N表示的是训练样例集中样本的数目。

  经验风险是对训练集中的所有样本点损失函数的平均最小化。经验风险越小说明模型f(x)对训练集的拟合程度越好，但是对于未知的样本效果怎么样呢？我们知道未知的样本数据(<X，Y>)的数量是不容易确定的，所以就没有办法用所有样本损失函数的平均值的最小化这个办法，那么怎么来衡量这个模型对所有的样本(包括位置的样本和已知的训练样本)预测能力呢？熟悉概率论表很容易想到期望。即假设X和Y服从联合分布P(X,Y)，那么期望风险就可以表示为：![](https://i.loli.net/2019/04/08/5caabb19ca728.png)

  这就是期望风险，期望风险表示的是全局的概念，表示的是决策函数对所有的样本<X,Y>预测能力的大小，而经验风险则是指的局部的概念，仅仅表示决策函数对训练数据集里样本的预测能力。理想的模型（决策）函数应该是让所有的样本的损失函数最小的（也即期望风险最小化），但是期望风https://i.loli.net/2019/04/08/5cab00c5dac7f.png险函数往往是不可得到的，即上式中，X与Y的联合分布函数不容易得到。现在我们已经清楚了期望风险是全局的，理想情况下应该让期望风险最小化，但是期望风险函数又不是那么容易得到的。怎么办呢？那就用局部最优的代替全局最优这个思想吧。这就是经验风险最小化的理论基础。

  **现在总结一下经验风险与期望风险之间的练习与区别**：

  经验风险是局部的，基于训练集所有样本点损失函数最小化的

  期望风险是全局的，是基于所有样本点的损失函数最小化的

  经验风险函数是现实的，可求得

  期望风险函数是理想化的，不可求的

  

  只考虑经验风险的话，会出现过拟合的现象，过拟合的极端情况便是模型f(x)对训练集中所有的样本点都有最好的预测能力，但是对于非训练集中的样本数据，模型的预测能力非常不好。怎么办呢？这个时候就引入了结构风险，结构风险是对经验风险和期望风险的折中。在经验风险函数后面加上一个正则化项(惩罚项)便是结构风险了。如下式：![](https://i.loli.net/2019/04/08/5caabb7c6144c.png)

  相比于经验风险，结构风险多了一个惩罚项，其中一个lamada是一个大于0的系数。J(f)表示的是模型f的复杂度。结构风险可以这么理解：

  结构风险越小，模型函数越复杂，其包含的参数越多，当经验风险函数小到一定程度就出现了过拟合现象。也可以理解为模型决策函数的复杂度是过拟合的必要条件，那么我们想要防止过拟合现象的方式，就是要破坏这个必要条件，即降低决策函数的复杂度。也即，让惩罚项J(f)最小化，现在出现两个需要最小化的函数了。我们需要同时保证经验风险函数和模型决策函数的复杂度都达到最小化，一个简单的办法是把两个式子融合成一个式子得到结构风险函数然后对这个风险函数进行最小化。

  

#### fast r-cnn之多任务损失

+ 首先给出fast里面损失函数数据结构

  ![](https://i.loli.net/2019/04/11/5cae970a3fb04.png)

  fast r-cnn网络里面有两个同级输出层(cls score和Bbox_predict层)，都是全连接层，称为multi-task。

  + clsscore层：用于分类，输出k+1维数组p，表示属于k类和背景的概率。对每个Rol(Region of Interesting)输出离散型概率分布：P = (P0，P1，……，Pk)，通常，k由k+1类的全连接层利用softmax计算得出。

  + bbox_predict层：用于调整候选区域位置，输出bounding box回归的位移，输出4*K维数组t，表示分别属于k类时，应该平移缩放的参数。

    

+ 损失函数记录暂定：给出讲解链接[Boke](<https://blog.csdn.net/qq_17448289/article/details/52871461>)

+ 损失函数对应到代码，给出链接[Boke](<https://blog.csdn.net/zhao347316568/article/details/85028216>)